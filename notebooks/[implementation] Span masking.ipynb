{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdb4f6b-8a70-4f92-b7cb-cb3f1d7a76e4",
   "metadata": {},
   "source": [
    "## Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a04b4bc-827d-455c-baf1-6a908b3abbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in /opt/conda/lib/python3.7/site-packages (1.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f1ec09-f191-416f-8a1a-89c66b1341a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0a117e-9d2c-49d8-af46-f173a8816380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args, data, mode):\n",
    "        self.data = data\n",
    "        self.data_dir = args.data_dir\n",
    "        self.mode = mode\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "        self.inputs, self.labels = self.data_loader()\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if os.path.isfile(os.path.join(self.data_dir, self.mode + '_X.pt')):\n",
    "            inputs = torch.load(os.path.join(self.data_dir, self.mode + '_X.pt'))\n",
    "            labels = torch.load(os.path.join(self.data_dir, self.mode + '_Y.pt'))\n",
    "\n",
    "        else:\n",
    "            df = self.data\n",
    "            inputs = pd.DataFrame(columns=['src'])\n",
    "            labels = pd.DataFrame(columns=['trg'])\n",
    "            inputs['src'] =  df['article_original']\n",
    "\n",
    "            if self.mode != \"test\":\n",
    "                labels['trg'] =  df['extractive']\n",
    "\n",
    "            # Preprocessing\n",
    "            inputs, labels = self.preprocessing(inputs, labels)\n",
    "            print(\"preprocessing\")\n",
    "\n",
    "            # Save data\n",
    "            torch.save(inputs, os.path.join(self.data_dir, self.mode + '_X.pt'))\n",
    "            torch.save(labels, os.path.join(self.data_dir, self.mode + '_Y.pt'))\n",
    "\n",
    "        inputs = inputs.values\n",
    "        labels = labels.values\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def pad(self, data, pad_id, max_len):\n",
    "        padded_data = data.map(lambda x : torch.cat([x, torch.tensor([pad_id] * (max_len - len(x)), dtype=torch.int64)]))\n",
    "        return padded_data\n",
    "    \n",
    "    def tokenize(self, x):\n",
    "        result = [self.tokenizer.encode(x[i], add_special_tokens=True) for i in range(len(x))]\n",
    "        result_concat = list(chain.from_iterable(result))\n",
    "        \n",
    "        if len(result_concat) <= 512:\n",
    "            return torch.tensor(result_concat)\n",
    "            \n",
    "        else:\n",
    "            length_sum = 0\n",
    "            for sen_token in result:\n",
    "                length_sum += len(sen_token)\n",
    "                \n",
    "            return torch.tensor(list(chain.from_iterable([self.tokenizer.encode(x[i], max_length = int(512 / len(x)), add_special_tokens=True) for i in range(len(x))])))\n",
    "\n",
    "\n",
    "    def preprocessing(self, inputs, labels):\n",
    "        print('Preprocessing ' + self.mode + ' dataset..')\n",
    "\n",
    "        # Encoding original text\n",
    "        inputs['src'] = inputs['src'].map(self.tokenize)\n",
    "        # inputs['src'] = inputs['src'].map(lambda x: torch.tensor(list(chain.from_iterable([self.tokenizer.encode(x[i], max_length = int(512 / len(x)), add_special_tokens=True) for i in range(len(x))]))))\n",
    "        inputs['clss'] = inputs.src.map(lambda x : torch.cat([torch.where(x == 2)[0], torch.tensor([len(x)])]))\n",
    "        inputs['segs'] = inputs.clss.map(lambda x : torch.tensor(list(chain.from_iterable([[0] * (x[i+1] - x[i]) if i % 2 == 0 else [1] * (x[i+1] - x[i]) for i, val in enumerate(x[:-1])]))))\n",
    "        inputs['clss'] = inputs.clss.map(lambda x : x[:-1])\n",
    "        \n",
    "        # Padding\n",
    "        max_encoding_len = max(inputs.src.map(lambda x: len(x)))\n",
    "        max_label_len = max(inputs.clss.map(lambda x: len(x)))\n",
    "        inputs['src'] = self.pad(inputs.src, 0, max_encoding_len)\n",
    "        inputs['segs'] = self.pad(inputs.segs, 0, max_encoding_len)\n",
    "        inputs['clss'] = self.pad(inputs.clss, -1, max_label_len)\n",
    "        inputs['mask'] = inputs.src.map(lambda x: ~ (x == 0))\n",
    "        inputs['mask_clss'] = inputs.clss.map(lambda x: ~ (x == -1))\n",
    "\n",
    "        # Binarize label {Extracted sentence : 1, Not Extracted sentence : 0}\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            labels = labels['trg'].map(lambda  x: torch.tensor([1 if i in x else 0 for i in range(max_label_len)]))\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'test':\n",
    "            return [self.inputs[index][i] for i in range(5)]\n",
    "        else:\n",
    "            return [self.inputs[index][i] for i in range(5)], self.labels[index]\n",
    "\n",
    "\n",
    "def get_train_loaders(args):\n",
    "    \"\"\"\n",
    "        define train/validation pytorch dataset & loader\n",
    "\n",
    "        Returns:\n",
    "            train_loader: pytorch data loader for train data\n",
    "            val_loader: pytorch data loader for validation data\n",
    "    \"\"\"\n",
    "    # get data from json\n",
    "    with open(os.path.join(args.data_dir, \"train.json\"), \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = pd.read_json(f) \n",
    "    train_df = pd.DataFrame(data)\n",
    "    \n",
    "    if args.train_kfold:\n",
    "        kf = KFold(n_splits=5)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "            if args.fold != fold:\n",
    "                continue\n",
    "            train_data = train_df.iloc[train_idx]\n",
    "            val_data = train_df.iloc[val_idx]        \n",
    "    else:\n",
    "        train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=args.seed)\n",
    "    \n",
    "    # get train & valid dataset from dataset.py\n",
    "    train_dataset = CustomDataset(args, train_data, mode='train')\n",
    "    val_dataset = CustomDataset(args, val_data, mode='valid')\n",
    "\n",
    "    # define data loader based on each dataset\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  num_workers=args.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle=True)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                                batch_size=args.batch_size,\n",
    "                                num_workers=args.num_workers,\n",
    "                                pin_memory=True,\n",
    "                                drop_last=False,\n",
    "                                shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62629093-ceab-42db-ba44-bb1711bc21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "# 설정\n",
    "config['seed'] = 981201\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config['data_dir'] = '/opt/ml/Legal-Document-Summarization/data'\n",
    "config['model_name'] = 'klue/bert-base'\n",
    "config['batch_size'] = 32\n",
    "config['num_workers']= 4\n",
    "config['train_kfold']= False\n",
    "\n",
    "\n",
    "args = easydict.EasyDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145d708e-ba3c-4189-99be-dd5968e6a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(args.data_dir, \"train.json\"), \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = pd.read_json(f) \n",
    "train_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43490160-6832-4b1d-8a8e-338ae1e73b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>extractive</th>\n",
       "      <th>article_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196524</td>\n",
       "      <td>[13, 14, 15]</td>\n",
       "      <td>[[1] 의용 부동산등기법 제35조 제1항,, 제60조 제1항은 등기를 신청함에는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106984</td>\n",
       "      <td>[1, 2, 4]</td>\n",
       "      <td>[가. 주택개량을 위하여 조합원들이 스스로 결성한 주택개량재개발조합이 실시하는 재개...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190919</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[금원의 목적 내지 성질상 국가나 지방자치단체와 특정인 사이에서만 수수,, 결제되어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110573</td>\n",
       "      <td>[0, 2, 5]</td>\n",
       "      <td>[가. 자동차대여업자의 직원으로서는 운전면허 없는 운전자가 위조된 운전면허증의 복사...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156698</td>\n",
       "      <td>[0, 2, 3]</td>\n",
       "      <td>[소외회사의 평리사 6명을 제쳐 놓고 대표이사 3명만의 결의에 의하여 동회사의 대표...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    extractive                                   article_original\n",
       "0  196524  [13, 14, 15]  [[1] 의용 부동산등기법 제35조 제1항,, 제60조 제1항은 등기를 신청함에는 ...\n",
       "1  106984     [1, 2, 4]  [가. 주택개량을 위하여 조합원들이 스스로 결성한 주택개량재개발조합이 실시하는 재개...\n",
       "2  190919     [1, 2, 3]  [금원의 목적 내지 성질상 국가나 지방자치단체와 특정인 사이에서만 수수,, 결제되어...\n",
       "3  110573     [0, 2, 5]  [가. 자동차대여업자의 직원으로서는 운전면허 없는 운전자가 위조된 운전면허증의 복사...\n",
       "4  156698     [0, 2, 3]  [소외회사의 평리사 6명을 제쳐 놓고 대표이사 3명만의 결의에 의하여 동회사의 대표..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adea0aa0-88a0-4d0b-b6d2-ce833a1533d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.DataFrame(columns=['src'])\n",
    "labels = pd.DataFrame(columns=['trg'])\n",
    "inputs['src'] =  train_df['article_original']\n",
    "labels['trg'] =  train_df['extractive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25ee71f-280b-434a-89db-422adeacd539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1] 의용 부동산등기법 제35조 제1항,, 제60조 제1항은 등기를 신청함에는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[가. 주택개량을 위하여 조합원들이 스스로 결성한 주택개량재개발조합이 실시하는 재개...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[금원의 목적 내지 성질상 국가나 지방자치단체와 특정인 사이에서만 수수,, 결제되어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[가. 자동차대여업자의 직원으로서는 운전면허 없는 운전자가 위조된 운전면허증의 복사...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[소외회사의 평리사 6명을 제쳐 놓고 대표이사 3명만의 결의에 의하여 동회사의 대표...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src\n",
       "0  [[1] 의용 부동산등기법 제35조 제1항,, 제60조 제1항은 등기를 신청함에는 ...\n",
       "1  [가. 주택개량을 위하여 조합원들이 스스로 결성한 주택개량재개발조합이 실시하는 재개...\n",
       "2  [금원의 목적 내지 성질상 국가나 지방자치단체와 특정인 사이에서만 수수,, 결제되어...\n",
       "3  [가. 자동차대여업자의 직원으로서는 운전면허 없는 운전자가 위조된 운전면허증의 복사...\n",
       "4  [소외회사의 평리사 6명을 제쳐 놓고 대표이사 3명만의 결의에 의하여 동회사의 대표..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fce8916-3da8-47b9-8d2b-013247d7e95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1] 의용 부동산등기법 제35조 제1항,',\n",
       " '제60조 제1항은 등기를 신청함에는 등기원인을 증명하는 서면을 제출하여야 하고,',\n",
       " '등기관리가 등기를 완료한 때에는 등기원인을 증명하는 서면',\n",
       " '또는 신청서 부본에 등기번호,',\n",
       " '신청서 수부(受附) 연월일,',\n",
       " '수부번호(受附番號),',\n",
       " '순위번호 및 등기제(登記濟)의 뜻을 기재하고,',\n",
       " '등기소의 인을 압날하여 이를 등기권리자에게 환부하여야 한다고 규정하고 있다.',\n",
       " '그러므로 매도증서에 위 규정에 따른 등기번호,',\n",
       " '등기순위, 등기제 등의 기재와 등기소인이 날인되어 있는 사실이 인정된다면,',\n",
       " '이는 등기 신청 시 등기원인을 증명하는 서면으로 제출되었다가 등기관리가 등기를 완료하고',\n",
       " '등기권리자에게 되돌려준 것으로 보지 않을 수 없다.',\n",
       " '따라서 특별한 사정이 없는 한 그 서면에 기재된 부동산에 관하여 그 기재의 등기번호와 순위번호에 따른 등기가 마쳐졌다고 인정하여야 한다.',\n",
       " '[2] 민사소송법 제356조 제1항은 문서의 작성방식과 취지에 의하여 공무원이 직무상 작성한 것으로 인정한 때에는 이를 진정한 공문서로 추정한다고 규정하고 있으나,',\n",
       " '위조 또는 변조 등 특별한 사정이 있다고 볼 만한 반증이 있는 경우에는 위와 같은 추정은 깨어진다.',\n",
       " '[3] 매도증서 등에 등기소의 등기제(登記濟)의 기재가 첨가됨으로써 사문서와 공문서로 구성된 문서는 공증에 관한 문서와는 달라 공문서 부분 성립이 인정된다고 하여 바로 사문서 부분인 매도증서 자체의 진정성립이 추정되거나 인정될 수는 없다.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455c7748-2b96-4a36-ad92-e720407f1f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[13, 14, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 2, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 2, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 2, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            trg\n",
       "0  [13, 14, 15]\n",
       "1     [1, 2, 4]\n",
       "2     [1, 2, 3]\n",
       "3     [0, 2, 5]\n",
       "4     [0, 2, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bd335-b9cd-4415-8d79-eff4e4d952ef",
   "metadata": {},
   "source": [
    "## Make word index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693f223a-3376-4997-8f8d-859833856993",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69c2f8f3-83d0-482d-8d40-2fcef3b52517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    result = [tokenizer.encode(x[i], add_special_tokens=True) for i in range(len(x))]\n",
    "    result_concat = list(chain.from_iterable(result))\n",
    "\n",
    "    if len(result_concat) <= 512:\n",
    "        return torch.tensor(result_concat)\n",
    "    \n",
    "    else:\n",
    "        length_sum = 0\n",
    "        for sen_token in result:\n",
    "            length_sum += len(sen_token)\n",
    "\n",
    "        return torch.tensor(list(chain.from_iterable([tokenizer.encode(x[i], max_length = int(512 * (len(result[i]) / length_sum)-1), add_special_tokens=True, truncation=True) for i in range(len(x))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dee17a8-ebb5-42b6-8c0d-25050fafa945",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['src'] = inputs['src'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3bd5f4-95cc-4155-9bda-3fcca35868b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[', '1', ']', '의', '##용', '부동산', '##등기', '##법', '제', '##35', '##조', '제', '##1', '##항', ',', '[SEP]', '[CLS]', '제', '##60', '##조', '제', '##1', '##항', '##은', '등기', '##를', '신청', '##함', '##에', '##는', '등기', '##원', '##인', '##을', '증명', '##하', '##는', '서면', '##을', '제출', '##하여', '##야', '하고', ',', '[SEP]', '[CLS]', '등기', '##관리', '##가', '등기', '##를', '완료', '##한', '때', '##에', '##는', '등기', '##원', '##인', '##을', '증명', '##하', '##는', '서면', '[SEP]', '[CLS]', '또는', '신청서', '부', '##본', '##에', '등기', '##번', '##호', ',', '[SEP]', '[CLS]', '신청서', '수부', '(', '受', '[UNK]', ')', '연', '##월일', ',', '[SEP]', '[CLS]', '수부', '##번', '##호', '(', '受', '[UNK]', '[UNK]', '[UNK]', ')', ',', '[SEP]', '[CLS]', '순위', '##번', '##호', '및', '등기', '##제', '(', '[UNK]', '記', '濟', ')', '의', '뜻', '##을', '기재', '##하고', ',', '[SEP]', '[CLS]', '등기', '##소', '##의', '인', '##을', '압', '##날', '##하여', '이를', '등기', '##권', '##리', '##자', '##에', '##게', '환', '##부', '##하여', '##야', '한다고', '규정', '##하고', '있', '##다', '.', '[SEP]', '[CLS]', '그러므로', '매도', '##증', '##서', '##에', '위', '규정', '##에', '따른', '등기', '##번', '##호', ',', '[SEP]', '[CLS]', '등기', '##순위', ',', '등기', '##제', '등', '##의', '기재', '##와', '등기', '##소', '##인', '##이', '날', '##인', '##되', '##어', '있', '##는', '사실', '##이', '인정', '##된', '##다면', ',', '[SEP]', '[CLS]', '이', '##는', '등기', '신청', '시', '등기', '##원', '##인', '##을', '증명', '##하', '##는', '서면', '##으로', '제출', '##되', '##었', '##다가', '등기', '##관리', '##가', '등기', '##를', '완료', '##하고', '[SEP]', '[CLS]', '등기', '##권', '##리', '##자', '##에', '##게', '되돌려', '##준', '것', '##으로', '보', '##지', '않', '##을', '수', '없', '##다', '.', '[SEP]', '[CLS]', '따라서', '특별', '##한', '사정', '##이', '없', '##는', '한', '그', '서면', '##에', '기재', '##된', '부동산', '##에', '관하', '##여', '그', '기재', '##의', '등기', '##번', '##호', '##와', '순위', '##번', '##호', '##에', '따른', '등기', '##가', '마쳐', '##졌', '##다고', '인정', '##하여', '##야', '한다', '.', '[SEP]', '[CLS]', '[', '2', ']', '민사', '##소송', '##법', '제', '##35', '##6', '##조', '제', '##1', '##항', '##은', '문서', '##의', '작성', '##방식', '##과', '취지', '##에', '의하', '##여', '공무원', '##이', '직무', '##상', '작성', '##한', '것', '##으로', '인정', '##한', '때', '##에', '##는', '이를', '진정한', '공문서', '##로', '추정', '##한다', '##고', '규정', '##하고', '있', '##으나', ',', '[SEP]', '[CLS]', '위조', '또는', '변조', '등', '특별', '##한', '사정', '##이', '있', '##다고', '볼', '만한', '반증', '##이', '있', '##는', '경우', '##에', '##는', '위', '##와', '같', '##은', '추정', '##은', '깨', '##어진다', '.', '[SEP]', '[CLS]', '[', '3', ']', '매도', '##증', '##서', '등', '##에', '등기', '##소', '##의', '등기', '##제', '(', '[UNK]', '記', '濟', ')', '의', '기재', '##가', '첨가', '##됨', '##으로', '##써', '사문', '##서', '##와', '공문서', '##로', '구성', '##된', '문서', '##는', '공', '##증', '##에', '관한', '문서', '##와', '##는', '달라', '공문서', '부분', '성립', '##이', '인정', '##된', '##다고', '하여', '바로', '사문', '##서', '부분', '##인', '매도', '##증', '##서', '자체', '##의', '진정', '##성', '##립', '##이', '추정', '##되', '##거나', '인정', '##될', '수', '##는', '없', '##다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs['src'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee05ad79-2712-4d83-a633-2e4622a4f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_index_dict(tokens):\n",
    "    word_index = {}\n",
    "    word = ''\n",
    "    index = []\n",
    "    \n",
    "    for i, t in enumerate(tokens):\n",
    "        if (t == '[CLS]') or (t == '[SEP]'):\n",
    "            continue\n",
    "        if not t.startswith('##'):\n",
    "            if word:\n",
    "                word_index[word.replace('##', '')] = index\n",
    "                word = ''\n",
    "                index = []\n",
    "            word += t\n",
    "            index.append(i)\n",
    "        if t.startswith('##'):\n",
    "            word += t\n",
    "            index.append(i)\n",
    "                \n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a097d9b3-ebb2-4a06-85da-83130161c1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[': [357], '1': [2], ']': [359], '의용': [4, 5], '부동산등기법': [6, 7, 8], '제35조': [9, 10, 11], '제1항': [12, 13, 14], ',': [324], '제60조': [18, 19, 20], '제1항은': [287, 288, 289, 290], '등기를': [210, 211], '신청함에는': [27, 28, 29, 30], '등기원인을': [194, 195, 196, 197], '증명하는': [198, 199, 200], '서면을': [38, 39], '제출하여야': [40, 41, 42], '하고': [43], '등기관리가': [207, 208, 209], '완료한': [52, 53], '때에는': [310, 311, 312], '서면': [64], '또는': [328], '신청서': [78], '부본에': [69, 70, 71], '등기번호': [156, 157, 158], '수부': [79], '(': [370], '受': [93], '[UNK]': [371], ')': [374], '연월일': [84, 85], '수부번호': [89, 90, 91], '순위번호': [101, 102, 103], '및': [104], '등기제': [368, 369], '記': [372], '濟': [373], '의': [375], '뜻을': [113, 114], '기재하고': [115, 116], '등기소의': [365, 366, 367], '인을': [123, 124], '압날하여': [125, 126, 127], '이를': [313], '등기권리자에게': [216, 217, 218, 219, 220, 221], '환부하여야': [135, 136, 137, 138], '한다고': [139], '규정하고': [320, 321], '있다': [142, 143], '.': [354], '그러므로': [147], '매도증서에': [148, 149, 150, 151], '위': [152], '규정에': [153, 154], '따른': [264], '등기순위': [162, 163], '등의': [167, 168], '기재와': [169, 170], '등기소인이': [171, 172, 173, 174], '날인되어': [175, 176, 177, 178], '있는': [341, 342], '사실이': [181, 182], '인정된다면': [183, 184, 185], '이는': [189, 190], '등기': [191], '신청': [192], '시': [193], '서면으로': [201, 202], '제출되었다가': [203, 204, 205, 206], '완료하고': [212, 213], '되돌려준': [222, 223], '것으로': [306, 307], '보지': [226, 227], '않을': [228, 229], '수': [230], '없다': [428, 429], '따라서': [236], '특별한': [331, 332], '사정이': [333, 334], '없는': [241, 242], '한': [243], '그': [253], '서면에': [245, 246], '기재된': [247, 248], '부동산에': [249, 250], '관하여': [251, 252], '기재의': [254, 255], '등기번호와': [256, 257, 258, 259], '순위번호에': [260, 261, 262, 263], '등기가': [265, 266], '마쳐졌다고': [267, 268, 269], '인정하여야': [270, 271, 272], '한다': [273], '2': [278], '민사소송법': [280, 281, 282], '제356조': [283, 284, 285, 286], '문서의': [291, 292], '작성방식과': [293, 294, 295], '취지에': [296, 297], '의하여': [298, 299], '공무원이': [300, 301], '직무상': [302, 303], '작성한': [304, 305], '인정한': [308, 309], '진정한': [314], '공문서로': [385, 386], '추정한다고': [317, 318, 319], '있으나': [322, 323], '위조': [327], '변조': [329], '등': [330], '있다고': [335, 336], '볼': [337], '만한': [338], '반증이': [339, 340], '경우에는': [343, 344, 345], '위와': [346, 347], '같은': [348, 349], '추정은': [350, 351], '깨어진다': [352, 353], '3': [358], '매도증서': [412, 413, 414], '등에': [363, 364], '기재가': [376, 377], '첨가됨으로써': [378, 379, 380, 381], '사문서와': [382, 383, 384], '구성된': [387, 388], '문서는': [389, 390], '공증에': [391, 392, 393], '관한': [394], '문서와는': [395, 396, 397], '달라': [398], '공문서': [399], '부분': [400], '성립이': [401, 402], '인정된다고': [403, 404, 405], '하여': [406], '바로': [407], '사문서': [408, 409], '부분인': [410, 411], '자체의': [415, 416], '진정성립이': [417, 418, 419, 420], '추정되거나': [421, 422, 423], '인정될': [424, 425], '수는': [426, 427]}\n",
      "\n",
      "['부동산등기법', '제35조', '제1항', '제60조', '제1항은', '등기를', '신청함에는', '등기원인을', '증명하는', '서면을', '제출하여야', '등기관리가', '완료한', '때에는', '신청서', '부본에', '등기번호', '[UNK]', '연월일', '수부번호', '순위번호', '등기제', '기재하고', '등기소의', '압날하여', '등기권리자에게', '환부하여야', '한다고', '규정하고', '그러므로', '매도증서에', '규정에', '등기순위', '기재와', '등기소인이', '날인되어', '사실이', '인정된다면', '서면으로', '제출되었다가', '완료하고', '되돌려준', '것으로', '따라서', '특별한', '사정이', '서면에', '기재된', '부동산에', '관하여', '기재의', '등기번호와', '순위번호에', '등기가', '마쳐졌다고', '인정하여야', '민사소송법', '제356조', '문서의', '작성방식과', '취지에', '의하여', '공무원이', '직무상', '작성한', '인정한', '진정한', '공문서로', '추정한다고', '있으나', '있다고', '반증이', '경우에는', '추정은', '깨어진다', '매도증서', '기재가', '첨가됨으로써', '사문서와', '구성된', '문서는', '공증에', '문서와는', '공문서', '성립이', '인정된다고', '사문서', '부분인', '자체의', '진정성립이', '추정되거나', '인정될']\n"
     ]
    }
   ],
   "source": [
    "for src in inputs['src']:\n",
    "    tokens = tokenizer.convert_ids_to_tokens(src)\n",
    "    word_dict = make_word_index_dict(tokens)\n",
    "    print(word_dict, end='\\n\\n')\n",
    "    sorted_dict = [key for key in list(word_dict.keys()) if len(key) > 2]\n",
    "    print(sorted_dict)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da6b73-1e90-48d8-b497-74a2213e52ed",
   "metadata": {},
   "source": [
    "## Masking to random tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb6fcd4b-2dae-4162-92ce-77702a129ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "504b5662-3c36-43f3-aa7e-a53ecc17a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_random_tokens(inputs, k=2):\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    \n",
    "    for src in inputs:\n",
    "        index_to_mask = []\n",
    "        \n",
    "        tokens = tokenizer.convert_ids_to_tokens(src)\n",
    "        word_dict = make_word_index_dict(tokens)\n",
    "        candidates = [key for key in list(word_dict.keys()) if len(key) > 2]\n",
    "        \n",
    "        for i in range(k):\n",
    "            rand_num = random.randint(0, len(candidates)-1)\n",
    "            index_to_mask.extend(word_dict[candidates[rand_num]])\n",
    "            \n",
    "        for idx in index_to_mask:\n",
    "            src[idx] = mask_token_id\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9eb4678-c103-444c-859d-4b0fe690acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_long_tokens(inputs, k=2):\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    \n",
    "    for src in inputs:\n",
    "        index_to_mask = []\n",
    "        \n",
    "        tokens = tokenizer.convert_ids_to_tokens(src)\n",
    "        word_dict = make_word_index_dict(tokens)\n",
    "        candidates = sorted(list(word_dict.keys()), reverse=True, key=len)\n",
    "        \n",
    "        for i in range(k):\n",
    "            index_to_mask.extend(word_dict[candidates[i]])\n",
    "            \n",
    "        for idx in index_to_mask:\n",
    "            src[idx] = mask_token_id\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef93a5b2-babc-4122-a545-4f87b43511d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 s, sys: 0 ns, total: 39.3 s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs['src'] = mask_to_long_tokens(inputs['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d437d08-497b-4834-9b94-2d01125074f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[', '1', ']', '의', '##용', '[MASK]', '[MASK]', '[MASK]', '제', '##35', '##조', '제', '##1', '##항', ',', '[SEP]', '[CLS]', '제', '##60', '##조', '제', '##1', '##항', '##은', '등기', '##를', '신청', '##함', '##에', '##는', '등기', '##원', '##인', '##을', '증명', '##하', '##는', '서면', '##을', '제출', '##하여', '##야', '하고', ',', '[SEP]', '[CLS]', '등기', '##관리', '##가', '등기', '##를', '완료', '##한', '때', '##에', '##는', '등기', '##원', '##인', '##을', '증명', '##하', '##는', '서면', '[SEP]', '[CLS]', '또는', '신청서', '부', '##본', '##에', '등기', '##번', '##호', ',', '[SEP]', '[CLS]', '신청서', '수부', '(', '受', '[UNK]', ')', '연', '##월일', ',', '[SEP]', '[CLS]', '수부', '##번', '##호', '(', '受', '[UNK]', '[UNK]', '[UNK]', ')', ',', '[SEP]', '[CLS]', '순위', '##번', '##호', '및', '등기', '##제', '(', '[UNK]', '記', '濟', ')', '의', '뜻', '##을', '기재', '##하고', ',', '[SEP]', '[CLS]', '등기', '##소', '##의', '인', '##을', '압', '##날', '##하여', '이를', '등기', '##권', '##리', '##자', '##에', '##게', '환', '##부', '##하여', '##야', '한다고', '규정', '##하고', '있', '##다', '.', '[SEP]', '[CLS]', '그러므로', '매도', '##증', '##서', '##에', '위', '규정', '##에', '따른', '등기', '##번', '##호', ',', '[SEP]', '[CLS]', '등기', '##순위', ',', '등기', '##제', '등', '##의', '기재', '##와', '등기', '##소', '##인', '##이', '날', '##인', '##되', '##어', '있', '##는', '사실', '##이', '인정', '##된', '##다면', ',', '[SEP]', '[CLS]', '이', '##는', '등기', '신청', '시', '등기', '##원', '##인', '##을', '증명', '##하', '##는', '서면', '##으로', '제출', '##되', '##었', '##다가', '등기', '##관리', '##가', '등기', '##를', '완료', '##하고', '[SEP]', '[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '되돌려', '##준', '것', '##으로', '보', '##지', '않', '##을', '수', '없', '##다', '.', '[SEP]', '[CLS]', '따라서', '특별', '##한', '사정', '##이', '없', '##는', '한', '그', '서면', '##에', '기재', '##된', '부동산', '##에', '관하', '##여', '그', '기재', '##의', '등기', '##번', '##호', '##와', '순위', '##번', '##호', '##에', '따른', '등기', '##가', '마쳐', '##졌', '##다고', '인정', '##하여', '##야', '한다', '.', '[SEP]', '[CLS]', '[', '2', ']', '민사', '##소송', '##법', '제', '##35', '##6', '##조', '제', '##1', '##항', '##은', '문서', '##의', '작성', '##방식', '##과', '취지', '##에', '의하', '##여', '공무원', '##이', '직무', '##상', '작성', '##한', '것', '##으로', '인정', '##한', '때', '##에', '##는', '이를', '진정한', '공문서', '##로', '추정', '##한다', '##고', '규정', '##하고', '있', '##으나', ',', '[SEP]', '[CLS]', '위조', '또는', '변조', '등', '특별', '##한', '사정', '##이', '있', '##다고', '볼', '만한', '반증', '##이', '있', '##는', '경우', '##에', '##는', '위', '##와', '같', '##은', '추정', '##은', '깨', '##어진다', '.', '[SEP]', '[CLS]', '[', '3', ']', '매도', '##증', '##서', '등', '##에', '등기', '##소', '##의', '등기', '##제', '(', '[UNK]', '記', '濟', ')', '의', '기재', '##가', '첨가', '##됨', '##으로', '##써', '사문', '##서', '##와', '공문서', '##로', '구성', '##된', '문서', '##는', '공', '##증', '##에', '관한', '문서', '##와', '##는', '달라', '공문서', '부분', '성립', '##이', '인정', '##된', '##다고', '하여', '바로', '사문', '##서', '부분', '##인', '매도', '##증', '##서', '자체', '##의', '진정', '##성', '##립', '##이', '추정', '##되', '##거나', '인정', '##될', '수', '##는', '없', '##다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs['src'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfb6a4-fb59-47da-b453-9c56dbcb335a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
