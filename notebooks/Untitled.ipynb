{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a04b4bc-827d-455c-baf1-6a908b3abbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in /opt/conda/lib/python3.7/site-packages (1.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f1ec09-f191-416f-8a1a-89c66b1341a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0a117e-9d2c-49d8-af46-f173a8816380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args, data, mode):\n",
    "        self.data = data\n",
    "        self.data_dir = args.data_dir\n",
    "        self.mode = mode\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "        self.inputs, self.labels = self.data_loader()\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if os.path.isfile(os.path.join(self.data_dir, self.mode, self.mode + '_X.pt')):\n",
    "            inputs = torch.load(os.path.join(self.data_dir, self.mode, self.mode + '_X.pt'))\n",
    "            labels = torch.load(os.path.join(self.data_dir, self.mode, self.mode + '_Y.pt'))\n",
    "        \n",
    "        else:\n",
    "            df = self.data\n",
    "            inputs = pd.DataFrame(columns=['src'])\n",
    "            labels = pd.DataFrame(columns=['trg'])\n",
    "            inputs['src'] =  df['article_original']\n",
    "#             print(inputs['src'])\n",
    "\n",
    "            if self.mode != \"test\":\n",
    "                labels['trg'] =  df['extractive']\n",
    "                \n",
    "#             print(labels)\n",
    "\n",
    "            # Preprocessing\n",
    "            inputs, labels = self.preprocessing(inputs, labels)\n",
    "            print(\"preprocessing\")\n",
    "\n",
    "            # Save data\n",
    "            torch.save(inputs, os.path.join(self.data_dir, self.mode, self.mode + '_X.pt'))\n",
    "            torch.save(labels, os.path.join(self.data_dir, self.mode, self.mode + '_Y.pt'))\n",
    "\n",
    "        inputs = inputs.values\n",
    "        labels = labels.values\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def pad(self, data, pad_id, max_len):\n",
    "        print(data[0])\n",
    "        print(type(pad_id))\n",
    "        padded_data = data.map(lambda x : torch.cat([x, torch.tensor([pad_id] * (max_len - len(x)))]))\n",
    "        return padded_data\n",
    "\n",
    "    def preprocessing(self, inputs, labels):\n",
    "        print('Preprocessing ' + self.mode + ' dataset..')\n",
    "\n",
    "        # Encoding original text\n",
    "        inputs['src'] = inputs['src'].map(lambda x: torch.tensor(list(chain.from_iterable([self.tokenizer.encode(x[i], max_length = int(512 / len(x)), add_special_tokens=True) for i in range(len(x))]))))\n",
    "        inputs['clss'] = inputs.src.map(lambda x : torch.cat([torch.where(x == 2)[0], torch.tensor([len(x)])]))\n",
    "        inputs['segs'] = inputs.clss.map(lambda x : torch.tensor(list(chain.from_iterable([[0] * (x[i+1] - x[i]) if i % 2 == 0 else [1] * (x[i+1] - x[i]) for i, val in enumerate(x[:-1])]))))\n",
    "        inputs['clss'] = inputs.clss.map(lambda x : x[:-1])\n",
    "        \n",
    "        #dtype = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "        #inputs['src'] = torch.LongTensor(torch.from_numpy(inputs['src'].values).type(dtype))\n",
    "        \n",
    "#         print(inputs.src)\n",
    "#         print(inputs.src.values)\n",
    "        # Padding\n",
    "        max_encoding_len = max(inputs.src.map(lambda x: len(x)))\n",
    "        max_label_len = max(inputs.clss.map(lambda x: len(x)))\n",
    "        inputs['src'] = self.pad(inputs.src, 0, max_encoding_len)\n",
    "        inputs['segs'] = self.pad(inputs.segs, 0, max_encoding_len)\n",
    "        inputs['clss'] = self.pad(inputs.clss, -1, max_label_len)\n",
    "        inputs['mask'] = inputs.src.map(lambda x: ~ (x == 0))\n",
    "        inputs['mask_clss'] = inputs.clss.map(lambda x: ~ (x == -1))\n",
    "\n",
    "        # Binarize label {Extracted sentence : 1, Not Extracted sentence : 0}\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            labels = labels['trg'].map(lambda  x: torch.tensor([1 if i in x else 0 for i in range(max_label_len)]))\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'test':\n",
    "            return [self.inputs[index][i] for i in range(5)]\n",
    "        else:\n",
    "            return [self.inputs[index][i] for i in range(5)], self.labels[index]\n",
    "\n",
    "\n",
    "def get_train_loaders(args):\n",
    "    \"\"\"\n",
    "        define train/validation pytorch dataset & loader\n",
    "\n",
    "        Returns:\n",
    "            train_loader: pytorch data loader for train data\n",
    "            val_loader: pytorch data loader for validation data\n",
    "    \"\"\"\n",
    "    # Get data from json\n",
    "#     with open(os.path.join(args.data_dir, \"train.json\"), \"r\", encoding=\"utf-8-sig\") as f:\n",
    "#         data = pd.read_json(f) \n",
    "#     train_df = pd.DataFrame(data)\n",
    "    path = os.path.join(args.data_dir, \"train.json\")\n",
    "    train_df = pd.read_json(path, orient='records', encoding='utf-8-sig')\n",
    "    \n",
    "    # Split train & test data\n",
    "    train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=args.seed)\n",
    "    \n",
    "    # Get train & valid dataset from dataset.py\n",
    "    train_dataset = CustomDataset(args, train_data, mode='train')\n",
    "    \n",
    "    val_dataset = CustomDataset(args, val_data, mode='valid')\n",
    "\n",
    "    # Define data loader based on each dataset\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  num_workers=args.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle=True)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                                batch_size=args.batch_size,\n",
    "                                num_workers=args.num_workers,\n",
    "                                pin_memory=True,\n",
    "                                drop_last=False,\n",
    "                                shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "    \n",
    "    \n",
    "def get_test_loader(args):\n",
    "    # Get data from json\n",
    "    with open(os.path.join(args.data_dir, \"test.json\"), \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = pd.read_json(f) \n",
    "    test_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Load dataset & dataloader\n",
    "    test_dataset = CustomDataset(args, test_df, mode='test')\n",
    "    test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=args.batch_size,\n",
    "                                 num_workers=args.num_workers,\n",
    "                                 pin_memory=True,\n",
    "                                 drop_last=False,\n",
    "                                 shuffle=False)\n",
    "    \n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95cfedcf-31c8-4f12-bed0-e1dddd884547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "Preprocessing train dataset..\n",
      "tensor([    2,    61,    21,    63,  2730,  4374,  8463,  4202,  4036,  4190,\n",
      "        16977,  4578,  4180, 12876,  4651,    16,     3,     2,  2843, 15153,\n",
      "         4180, 12876,  4651,  4192, 30407,  4023, 12539, 17565,  4082, 30407,\n",
      "         4195,  9576, 11003,  7974, 30087,  4053, 13884, 24794,  8048,    16,\n",
      "            3,     2, 30407, 12159,  4050, 30407,  4023, 27821,  4069,  1074,\n",
      "         8180, 30407,  4195,  9576, 11003,  7974, 30087,     3,     2, 11707,\n",
      "        32682,  1776,  4065,  4063, 30407, 15466,    16,     3,     2, 32682,\n",
      "         2153,  4232,    12, 25453, 26762,    13,  2544, 49026,    16,     3,\n",
      "            2,  2153,  4232, 15466,    12, 25453, 26762, 26225, 26507,    13,\n",
      "           16,     3,     2, 23873, 15466,  1616, 30407,  4067,    12, 26239,\n",
      "        26553, 26147,    13,  2730, 14628, 18368,  7977,    16,     3,     2,\n",
      "        30407,  4194,  4041,  2744,  4053,  2445,  4517,  8457, 12047, 30407,\n",
      "         4080,  9894,  8026,  3893,  4232, 24794,  9046, 16744,  7977,  8130,\n",
      "           18,     3,     2, 28150, 13748,  4193, 14050,  2681, 16744,  4063,\n",
      "        15782, 30407, 15466,    16,     3,     2, 30407, 14781,    16, 30407,\n",
      "         4067, 20506, 18368,  4331, 30407,  4194,  8113, 36449,  9076,  8039,\n",
      "        11962,  8388, 20263,    16,     3,     2, 15811, 30407, 12539,  2233,\n",
      "        30407,  4195,  9576, 11003,  7974, 30087,  7973, 13884, 14206,  4050,\n",
      "        30407, 12159,  4050, 30407,  4023, 27821,  7977,     3,     2, 30407,\n",
      "         4080,  9894,  8026, 35160,  4241, 11934, 11645, 10098,  2153,  8137,\n",
      "           18,     3,     2, 14543, 24300, 17891,  4012,  8101,  3777,   363,\n",
      "        30087,  4063, 18368,  4132,  8463,  4063, 31628,  4197,   363, 18368,\n",
      "         4041, 30407, 15466,  4331, 23873, 15466,  4063, 15782, 30407,  4050,\n",
      "        42649, 15259,  8388,     3,     2,    61,    22,    63, 31393, 19950,\n",
      "         4190, 16977, 44409,  4180, 12876,  4651,  4192, 18774,  4041, 14488,\n",
      "        24971,  4169, 27769,  4063, 27680,  4197, 13689, 13774,  4156, 14488,\n",
      "         4069, 11934,  8388,  4069,  1074,     3,     2, 11119, 11707, 38563,\n",
      "         1031, 24300, 17891,  4012,  9026,  1735, 19697, 24084,  4012,  8039,\n",
      "         9877,  8180,  2681,  4331,  8069, 24236,  4192, 14219,  8780,    18,\n",
      "            3,     2,    61,    23,    63, 13748,  4193,  4033, 14898, 30407,\n",
      "         4194,  4041, 30407,  4067,    12, 26239, 26553, 26147,    13,  2730,\n",
      "        18368,  4050, 31098,  4226, 12885,  1982, 14466,  4331, 45745,  4096,\n",
      "        17201,  4132,     3])\n",
      "<class 'int'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for sequence element 1 in sequence argument at position #1 'tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5656b350c1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-411302574eb7>\u001b[0m in \u001b[0;36mget_train_loaders\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Get train & valid dataset from dataset.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-411302574eb7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, data, mode)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-411302574eb7>\u001b[0m in \u001b[0;36mdata_loader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preprocessing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-411302574eb7>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mmax_encoding_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mmax_label_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_encoding_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_encoding_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_label_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-411302574eb7>\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, data, pad_id, max_len)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mpadded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   3907\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3908\u001b[0m         \"\"\"\n\u001b[0;32m-> 3909\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3910\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   3911\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-411302574eb7>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mpadded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for sequence element 1 in sequence argument at position #1 'tensors'"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_loaders(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62629093-ceab-42db-ba44-bb1711bc21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "# 설정\n",
    "config['seed'] = 981201\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config['data_dir'] = '/opt/ml/Legal-Document-Summarization/data'\n",
    "config['model_name'] = 'beomi/KcELECTRA-base'\n",
    "config['batch_size'] = 64\n",
    "config['num_workers']=1\n",
    "\n",
    "\n",
    "args = easydict.EasyDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bfe874-00d8-4384-95d0-372ebba09bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "        self.inputs, self.labels = self.data_loader()\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if os.path.isfile(os.path.join(self.data_dir, self.mode, self.mode + '_X.pt')):\n",
    "            inputs = torch.load(os.path.join(self.data_dir, self.mode, self.mode + '_X.pt'))\n",
    "            labels = torch.load(os.path.join(self.data_dir, self.mode, self.mode + '_Y.pt'))\n",
    "\n",
    "        else:\n",
    "            file_path = os.path.join(self.data_dir, self.mode, self.mode + '.json')\n",
    "            df = utils.load_json(file_path)\n",
    "            inputs = pd.DataFrame(columns=['src'])\n",
    "            labels = pd.DataFrame(columns=['trg'])\n",
    "            inputs['src'] =  df['article_original']\n",
    "            labels['trg'] =  df['extractive']\n",
    "            # Preprocessing\n",
    "            inputs, labels = self.preprocessing(inputs, labels)\n",
    "            # Save data\n",
    "            torch.save(inputs ,os.path.join(self.data_dir, self.mode, self.mode + '_X.pt'))\n",
    "            torch.save(labels, os.path.join(self.data_dir, self.mode, self.mode + '_Y.pt'))\n",
    "\n",
    "        inputs = inputs.values\n",
    "        labels = labels.values\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def pad(self, data, pad_id, max_len):\n",
    "        padded_data = data.map(lambda x : torch.cat([x, torch.tensor([pad_id] * (max_len - len(x)))]))\n",
    "        return padded_data\n",
    "\n",
    "    def preprocessing(self, inputs, labels):\n",
    "        print('Preprocessing ' + self.mode + ' dataset..')\n",
    "\n",
    "        #Encoding original text\n",
    "        inputs['src'] = inputs['src'].map(lambda x: torch.tensor(list(chain.from_iterable([self.tokenizer.encode(x[i], max_length = int(512 / len(x)),  add_special_tokens=True) for i in range(len(x))]))))\n",
    "        inputs['clss'] = inputs.src.map(lambda x : torch.cat([torch.where(x == 2)[0], torch.tensor([len(x)])]))\n",
    "        inputs['segs'] = inputs.clss.map(lambda x : torch.tensor(list(chain.from_iterable([[0] * (x[i+1] - x[i]) if i % 2 == 0 else [1] * (x[i+1] - x[i]) for i, val in enumerate(x[:-1])]))))\n",
    "        inputs['clss'] = inputs.clss.map(lambda x : x[:-1])\n",
    "        # ##Padding\n",
    "        max_encoding_len = max(inputs.src.map(lambda x: len(x)))\n",
    "        max_label_len = max(inputs.clss.map(lambda x: len(x)))\n",
    "        inputs['src'] = self.pad(inputs.src, 0, max_encoding_len)\n",
    "        inputs['segs'] = self.pad(inputs.segs, 0, max_encoding_len)\n",
    "        inputs['clss'] = self.pad(inputs.clss, -1, max_label_len)\n",
    "        inputs['mask'] = inputs.src.map(lambda x: ~ (x == 0))\n",
    "        inputs['mask_clss'] = inputs.clss.map(lambda x: ~ (x == -1))\n",
    "\n",
    "        # #Binarize label {Extracted sentence : 1, Not Extracted sentence : 0}\n",
    "        labels = labels['trg'].map(lambda  x: torch.tensor([1 if i in x else 0 for i in range(max_label_len)]))\n",
    "        return inputs, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.inputs[index][i] for i in range(5)], self.labels[index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
